import logging
from pathlib import Path
from typing import Optional

import numpy as np
import torch
from gymnasium.wrappers import TransformObservation
from stable_baselines3.common import env_util
from stable_baselines3.common.atari_wrappers import AtariWrapper
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.vec_env.base_vec_env import (
    VecEnv,
    VecEnvObs,
    VecEnvStepReturn,
    VecEnvWrapper,
)
from torch import nn

from bayesianrex import constants
from bayesianrex.models.reward_model import RewardNetwork

logger = logging.getLogger(__name__)


# NOTE grayscale image normalization (/ 255) occurs in ActorCriticCnnPolicy.extract_features()
def mask_score(obs: np.ndarray, env_name: str) -> np.ndarray:
    """
    Obfuscates the score and life count pixel portions of some Atari 2600 games.

    :param obs: The original observation returned by the Atari Gymnasium
        environment with shape (H,W,C).
    :param env_name: A string representing the Atari game id as found in
        `bayesianrex.code.rl_utils.environments_mapper`.
    :return: The original observation without game score/life information.
    """
    obs_copy = obs.copy()
    if env_name in ["spaceinvaders", "breakout", "pong", "montezumarevenge"]:
        obs_copy[:10, :, :] = 0
    elif env_name == "beamrider":
        obs_copy[:16, :, :] = 0
        obs_copy[-11:, :, :] = 0
    elif env_name == "enduro":
        obs_copy[-14:, :, :] = 0
    elif env_name == "hero":
        obs_copy[-30:, :, :] = 0
    # elif env_name == "qbert":
    #     obs_copy[:12, :, :] = 0
    elif env_name == "seaquest":
        # cuts out divers and oxygen
        obs_copy[:12, :, :] = 0
        obs_copy[-16:, :, :] = 0
    # elif env_name == "mspacman":
    #     obs_copy[-15:, :, :] = 0  # mask score and number of lives left
    # elif env_name == "videopinball":
    #     obs_copy[:15, :, :] = 0
    else:
        logger.warning("Don't know how to mask '%s', pass", env_name)
    return obs_copy


def create_atari_env(
    env_id: str,
    n_envs: int = 1,
    seed: Optional[int] = None,
    wrapper_kwargs: Optional[dict] = None,
    **kwargs
) -> VecFrameStack:
    """
    Wrapper over sb3's make_atari_env to create stacks of 4 frames.

    :param env_id: environment ID
    :param n_envs: number of parallel environments to make
    :param seed: random seed for make_atari_env
    :param wrapper_kwargs: other arguments needed for make_atari_env
    :return: stack of frames generated by the atari environment
    """
    logger.debug("Env: %s", env_id)
    return VecFrameStack(
        env_util.make_atari_env(
            env_id, n_envs=n_envs, seed=seed, wrapper_kwargs=wrapper_kwargs, **kwargs
        ),
        n_stack=4,
    )


def create_hidden_lives_atari_env(
    env_name: str,
    n_envs: int = 1,
    seed: Optional[int] = None,
    atari_kwargs: Optional[dict] = None,
    **kwargs
) -> VecFrameStack:
    """
    Wrapper over sb3's make_atari_env to create stacks of 4 frames with the
    number of remaining lives masked with zeros.

    :param env_id: environment ID
    :param n_envs: number of parallel environments to make
    :param seed: random seed for make_atari_env
    :param atari_kwargs: other arguments needed for make_atari_env
    :return: stack of frames generated by the atari environment w/o lives
    """
    env_id = constants.envs_id_mapper.get(env_name)
    logger.info("Env name %s Env id %s", env_name, env_id)
    return VecFrameStack(
        env_util.make_vec_env(
            env_id,
            n_envs=n_envs,
            seed=seed,
            **kwargs,
            wrapper_class=lambda env, **_: TransformObservation(
                AtariWrapper(env, **(atari_kwargs or {})),
                lambda obs: mask_score(obs, env_name),
            ),
        ),
        n_stack=4,
    )


class MAPRewardWrapper(VecEnvWrapper):
    """
    Wrapper over sb3's Atari environments with a custom reward function,
    parameterised by a neural network. The reward is the MAP of the learned function.
    """

    def __init__(self, venv: VecEnv, reward_model: RewardNetwork, device):
        """
        Initializer
        :param venv: VectorEnvironment (i.e. sb3 Atari env) instance
        :param reward_model: custom pretrained reward function network
        """
        super().__init__(venv=venv)
        # NOTE necessary to avoid reparameterization trick in
        # RewardNetwork.cum_return(), as in original code (EmbeddingNet of
        # custom_reward_wrapper.py in baselines)
        self.reward_model = reward_model.eval().to(device)
        self.device = device

    def reset(self) -> VecEnvObs:
        """Reset the environment"""
        return self.venv.reset()

    def step_wait(self) -> VecEnvStepReturn:
        """Step through the environment and return env information"""
        obs, _, done, info = self.venv.step_wait()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # reward embeddings were trained on float representation of pixels
        obs = torch.Tensor(obs / 255.0).to(device)
        self.reward_model = self.reward_model.to(device)
        reward = self.reward_model.cum_return(obs)[0].detach().cpu().numpy()
        obs = obs.detach().cpu().numpy()
        # unsqueeze reward as returned by gymnasium API
        return (obs, reward[..., None], done, info)


class MeanRewardWrapper(MAPRewardWrapper):
    def __init__(self, venv: VecEnv, reward_model: RewardNetwork, chain_path: Path, device):
        super().__init__(venv, reward_model, device)
        # load MCMC data
        self.device = device
        mcmc_data = np.load(chain_path)
        mcmc_chain = mcmc_data["chain"]
        # last layer just outputs the scalar reward = w^T \phi(s)
        self.reward_model.trex = nn.Linear(reward_model.trex.in_features, 1, bias=False)
        # get average across (downsampled) chain and use it as the last layer in
        # the network
        burn, skip = int(5e3), 20
        if len(mcmc_chain) >= burn + 100:
            logger.info(
                "No proposal burning/downsampling, chain size: %d", len(mcmc_chain)
            )
            burn, skip = 0, 1
        mean_reward_fn = mcmc_chain[burn::skip].mean(0)
        mean_reward_fn = (
            torch.from_numpy(mean_reward_fn[None, ...])
            .to(torch.float32)  # default torch dtype
            .to(self.device)
        )
        self.reward_model.trex.weight = nn.Parameter(mean_reward_fn)
        self.reward_model = self.reward_model.to(device)
        logger.info("Succesfully set learned reward fn as mean of MCMC chain")
